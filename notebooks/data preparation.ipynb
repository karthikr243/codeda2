{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z0ZlyKDuyABB",
        "outputId": "dd4b8a9c-3bf0-49f8-825c-986fa97344b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'capstone_data_new.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bb5702880e3d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'capstone_data_new.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'capstone_data_new.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv('capstone_data_new.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": false,
        "id": "944rQLAIyABC",
        "outputId": "b5b0fb91-7f23-474d-d240-1c6eae996bf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-eda2b5cc1a42>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
          ]
        }
      ],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSt4N6hzyABD"
      },
      "outputs": [],
      "source": [
        "df1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_HGgqO6yABD"
      },
      "source": [
        "### Segregating into short and long texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "mqYhoB8QyABE"
      },
      "outputs": [],
      "source": [
        "df1[df1.length<280]['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bYep52njyABF",
        "outputId": "1b709f33-a69c-4ffc-e7e0-165ce2e8e7a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-84e546ee9115>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m280\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
          ]
        }
      ],
      "source": [
        "df1[df1.length>280]['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPNEdnm8yABF"
      },
      "outputs": [],
      "source": [
        "df_short = df1[df1.length<280][['text', \"label\"]]\n",
        "df_long = df1[df1.length>280][['text', \"label\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q32pawi4yABG"
      },
      "source": [
        "### Working with short texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Fo5nso4AyABG"
      },
      "outputs": [],
      "source": [
        "df_short.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_PxItEA3yABG"
      },
      "outputs": [],
      "source": [
        "df_short.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2iC7WbpyABG"
      },
      "outputs": [],
      "source": [
        "df_short.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yigze5S3yABH"
      },
      "source": [
        "We need to represent data as numeric values for the model. We need to do something similar for the textual information from the *text* column, but as this is dependent of the model architecture, this is done in the subsequent notebook.\n",
        "\n",
        "#### Real is 1 while Fake is 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O8-ZNBLcyABH",
        "outputId": "b4efcbc1-537e-48db-ffd7-aaa08ae6bbe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_short' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8985d8e29c0a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_short\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_short\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_short' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "enc = LabelEncoder()\n",
        "df_short['label'] = enc.fit_transform(df_short['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "b7LOC63GyABH"
      },
      "outputs": [],
      "source": [
        "df_short.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73xyOhuXyABH"
      },
      "outputs": [],
      "source": [
        "df_short.iloc[4]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHADeaP9yABH"
      },
      "outputs": [],
      "source": [
        "df_short.text.sample(10, random_state=1).to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20bWrymeyABH"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "1. Removed URLs\n",
        "2. Removed User Mentions\n",
        "3. Removed all numbers\n",
        "4. Removed punctuations and extra spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tG7uZtZAyABI"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "df_short['text'] = df_short['text'].apply(lambda x:re.sub(r'http\\S+', '', x))\n",
        "df_short['text'] = df_short['text'].apply(lambda x:re.sub(r'@\\S+ ', '', x))\n",
        "df_short['text'] = df_short['text'].apply(lambda x:''.join(i for i in x if not i.isdigit()))\n",
        "table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "df_short['text'] = df_short['text'].str.translate(table)\n",
        "df_short['text'] = df_short['text'].str.replace(' +', ' ')\n",
        "df_short['text'] = df_short['text'].str.lower()\n",
        "df_short['text'] = df_short['text'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "FR2JTSUMyABI"
      },
      "outputs": [],
      "source": [
        "df_short.text.sample(10, random_state=1).to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA7k5udGyABI"
      },
      "source": [
        "### Doubts\n",
        "* keep only english tweets? translate the non-eng ones to eng?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATvF7T-LyABI"
      },
      "outputs": [],
      "source": [
        "#Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_short.text\n",
        "y = df_short.label\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "rUER8NcgyABI"
      },
      "outputs": [],
      "source": [
        "df_short_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_short_test = pd.concat([X_test, y_test], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "01Xscno-yABI"
      },
      "outputs": [],
      "source": [
        "# df_short_test = df_short[~df_short.index.isin(df_short_train.index)][df_short.label==1]\n",
        "# df_short_test = pd.concat([df_short_test, df_short[~df_short.index.isin(df_short_train.index)][df_short.label==0].sample(n = df_short_test.shape[0], random_state=1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUh2a1ywyABI"
      },
      "outputs": [],
      "source": [
        "lengths = [len(df_short_train.iloc[i]['text'].split()) for i in range(len(df_short_train))]\n",
        "print(max(lengths))\n",
        "print(min(lengths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zyM7gjYyABI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFwEljiHyABI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.median(lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IJeubDUyABI"
      },
      "outputs": [],
      "source": [
        "df_short_train = df_short_train[[l > 0 for l in lengths]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V0R-GpPyABJ"
      },
      "outputs": [],
      "source": [
        "df_short_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkmtrqQ8yABJ"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(df_short_train['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_ZJ4ChxyABJ"
      },
      "source": [
        "We then save the preprocessed dataset, and another one corresponding to a 10% sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maCpKctoyABJ"
      },
      "outputs": [],
      "source": [
        "df_short_train.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG9yVPqiyABJ"
      },
      "outputs": [],
      "source": [
        "df_short_train.to_csv('short_text_preprocessed_train.csv', index=False)\n",
        "df_short_test.to_csv('short_text_preprocessed_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RlbQiMYyABJ"
      },
      "outputs": [],
      "source": [
        "df_short_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJdpnN_JyABJ"
      },
      "outputs": [],
      "source": [
        "# df_short.sample(n=int(len(df_short)*0.1), random_state=111).to_csv('short_text_sampled.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfuTDJvgyABJ"
      },
      "source": [
        "## Working with long texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMKlonXtyABJ"
      },
      "outputs": [],
      "source": [
        "df_long.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HA1qSyiVyABK"
      },
      "outputs": [],
      "source": [
        "enc = LabelEncoder()\n",
        "df_long['label'] = enc.fit_transform(df_long['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxQsPMSYyABK"
      },
      "outputs": [],
      "source": [
        "df_long.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "usc7PeH0yABK"
      },
      "outputs": [],
      "source": [
        "df_long.text.sample(1, random_state=1).to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoxSN2lOyABK"
      },
      "outputs": [],
      "source": [
        "df_long['text'] = df_long['text'].apply(lambda x:re.sub(r'http\\S+', '', x))\n",
        "df_long['text'] = df_long['text'].apply(lambda x:re.sub(r'@\\S+ ', '', x))\n",
        "df_long['text'] = df_long['text'].apply(lambda x:''.join(i for i in x if not i.isdigit()))\n",
        "table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "df_long['text'] = df_long['text'].str.translate(table)\n",
        "df_long['text'] = df_long['text'].str.replace(' +', ' ')\n",
        "df_long['text'] = df_long['text'].str.lower()\n",
        "df_long['text'] = df_long['text'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvLxbTKhyABK"
      },
      "outputs": [],
      "source": [
        "df_long.text.sample(1, random_state=1).to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7bbdgINyABK"
      },
      "outputs": [],
      "source": [
        "#Train test split\n",
        "\n",
        "X = df_long.text\n",
        "y = df_long.label\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1, stratify=y)\n",
        "\n",
        "df_long_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_long_test = pd.concat([X_test, y_test], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8fb7Hq5yABK"
      },
      "outputs": [],
      "source": [
        "lengths = [len(df_long_train.iloc[i]['text'].split()) for i in range(len(df_long_train))]\n",
        "print(max(lengths))\n",
        "print(min(lengths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "sGtaUwySyABL"
      },
      "outputs": [],
      "source": [
        "plt.hist([l for l in lengths if l<5000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qkPvKndyABL"
      },
      "outputs": [],
      "source": [
        "np.median(lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XBoahOGyABL"
      },
      "outputs": [],
      "source": [
        "df_long_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paGXKnCayABL"
      },
      "outputs": [],
      "source": [
        "df_long_train = df_long_train[[l < 1000 for l in lengths]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAk3oc-PyABL"
      },
      "outputs": [],
      "source": [
        "df_long_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty52eewZyABL"
      },
      "outputs": [],
      "source": [
        "df_long_train.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIuPirjuyABL"
      },
      "outputs": [],
      "source": [
        "df_long_train.to_csv('long_text_preprocessed_train.csv', index=False)\n",
        "df_long_test.to_csv('long_text_preprocessed_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiIVVcRbyABL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1CVwvETyABL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "599IobMpyABM"
      },
      "outputs": [],
      "source": [
        "categories = ['ClaimFakeCOVID-19_tweets', 'ClaimRealCOVID-19_tweets', 'NewsFakeCOVID-19_tweets', 'NewsRealCOVID-19_tweets']\n",
        "filenames = ['politifact_fake', 'politifact_real', 'gossipcop_fake', 'gossipcop_real']\n",
        "\n",
        "for category, filename in zip(categories, filenames):\n",
        "    print(\"Working on category:\", category)\n",
        "    tweet_ids = []\n",
        "    for i, year in enumerate(['05-01-2020', '07-01-2020', '09-01-2020', '11-01-2020']):\n",
        "        if category+'.csv' in os.listdir(f'../CoAID/{year}'):\n",
        "            df = pd.read_csv(f'../CoAID/{year}/{category}.csv')\n",
        "            tweet_ids.extend(df.tweet_id.to_list())\n",
        "\n",
        "    final = pd.DataFrame({'id':[category], 'news_url':[''], 'title':[''], 'tweet_ids':'\\t'.join(map(str, tweet_ids))})\n",
        "    print(\"Saving it to filename: {}.csv\".format(filename))\n",
        "    final.to_csv(f'../CoAID/{filename}.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTwQfRk8yABM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "PATH = f'..\\\\FakeNewsNet\\\\code\\\\fakenewsnet_dataset\\\\'\n",
        "\n",
        "real_tweets = set()\n",
        "fake_tweets = set()\n",
        "\n",
        "for status in ['real', 'fake']:\n",
        "    for category in ['gossipcop', 'politifact']:\n",
        "        source = os.listdir(PATH+f'{category}\\\\{status}\\\\')[0]\n",
        "        DIR = PATH+f\"{category}\\\\{status}\\\\{source}\\\\tweets\\\\\"\n",
        "        print(DIR)\n",
        "        print(len(set(os.listdir(DIR))))\n",
        "        for file in os.listdir(DIR):\n",
        "            with open(DIR+file) as f:\n",
        "                text = f.read()\n",
        "                t = json.loads(text)\n",
        "                if status=='real':\n",
        "                    real_tweets.add(t.get('text'))\n",
        "                else:\n",
        "                    fake_tweets.add(t.get('text'))\n",
        "        print(\"Counts for real and fake tweets:\", (len(real_tweets), len(fake_tweets)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "dn_Gr2lZyABM"
      },
      "outputs": [],
      "source": [
        "df_final = pd.DataFrame({'text':list(real_tweets)+list(fake_tweets), 'label':([1]*len(real_tweets)) + ([0]*len(fake_tweets))})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwpKBg0RyABM"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "df_final['text'] = df_final['text'].apply(lambda x:re.sub(r'http\\S+', '', x))\n",
        "df_final['text'] = df_final['text'].apply(lambda x:re.sub(r'@\\S+ ', '', x))\n",
        "df_final['text'] = df_final['text'].apply(lambda x:''.join(i for i in x if not i.isdigit()))\n",
        "table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "df_final['text'] = df_final['text'].str.translate(table)\n",
        "df_final['text'] = df_final['text'].str.replace(' +', ' ')\n",
        "df_final['text'] = df_final['text'].str.lower()\n",
        "df_final['text'] = df_final['text'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faKH1SpcyABM"
      },
      "outputs": [],
      "source": [
        "df_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BqLlJXRyABM"
      },
      "outputs": [],
      "source": [
        "df_final.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYq1cygPyABM"
      },
      "outputs": [],
      "source": [
        "df_final.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5kA1KAiyABN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "p = 'tmp_dir/another_dir/saved_df.csv'\n",
        "print(os.path.join(*p.split('/')[:2]))\n",
        "os.makedirs(os.path.join(*p.split('/')[:2]), exist_ok=True)\n",
        "df_final.to_csv(p, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "uqa3PRtXyABN"
      },
      "outputs": [],
      "source": [
        "df_final.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MguzCKY8yABN"
      },
      "outputs": [],
      "source": [
        "pd.read_csv('data/shorttextpreprocessedtrain.csv').shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCpE0ihvyABN"
      },
      "outputs": [],
      "source": [
        "df_final_final = df_final.append(pd.read_csv('data/shorttextpreprocessedtrain.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNepVW-IyABN"
      },
      "outputs": [],
      "source": [
        "df_final_final = df_final_final.sample(frac = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo5UysGEyABN"
      },
      "outputs": [],
      "source": [
        "df_final_final.to_csv('data/newdatasetwithcoviddata.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2WDH0t4yABN"
      },
      "outputs": [],
      "source": [
        "df_final_final = pd.read_csv('data/newdatasetwithcoviddata.csv').dropna()\n",
        "df_final_final.to_csv('data/newdatasetwithcoviddata.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VutqUc-NyABN"
      },
      "outputs": [],
      "source": [
        "pd.read_csv('data/newdatasetwithcoviddata.csv').isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLskeMGSyABN"
      },
      "source": [
        "### Trying out multiprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAvZGmVnyABO"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import multiprocessing as mp\n",
        "# import time\n",
        "# import re\n",
        "# from nltk.corpus import stopwords\n",
        "# import string\n",
        "\n",
        "# t = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "\n",
        "# def clean_text(text):\n",
        "#     # Remove stop words\n",
        "#     stops = set(stopwords.words(\"english\"))\n",
        "#     text = \" \".join(list(set(text.lower().split()) - stops))\n",
        "#     # Remove Special Characters\n",
        "#     text = text.translate(t)\n",
        "#     # removing the extra spaces\n",
        "#     text = re.sub(' +',' ', text)\n",
        "#     return text\n",
        "\n",
        "# df = pd.read_csv(\"src/Blob_04_05_2021/request_2667/fileblock_0.csv\") # file loading\n",
        "# print(\"Columns of the dataset\", list(df.columns))\n",
        "# print(\"Total records of the dataset\", len(df))\n",
        "\n",
        "# # Before Parallel Processing\n",
        "# df1 = df.copy()\n",
        "# t1 = time.time()\n",
        "# df1['tweet'] = df1['tweet'].apply(clean_text)\n",
        "# t2 = time.time()\n",
        "# print(\"time consuming before Parallel Processing to process the Dataset {0:.2f}s\".format(round(t2-t1, 2)))\n",
        "\n",
        "# # After Parallel Processing\n",
        "# p = mp.Pool(mp.cpu_count()-1) # Data parallelism Object\n",
        "# df2 = df.copy()\n",
        "# t3 = time.time()\n",
        "# df2['tweet'] = p.map(clean_text, df2['tweet'])\n",
        "# t4 = time.time()\n",
        "\n",
        "# print(\"time consuming after Parallel Processing to process the Dataset {0:.2f}s\".format(round(t4-t3, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYqkA7RIyABO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}